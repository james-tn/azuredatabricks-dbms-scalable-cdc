{"cells":[{"cell_type":"markdown","source":["##Template for performing event based ingestion and merging from Attunity change files"],"metadata":{}},{"cell_type":"markdown","source":["Scalable CDC from DBMS to Azure Databricks\n\n1. Summary of algorithm\n\n  - CDC program send change data from various tables into ADLS Gen 2 folder, each table has its own folder.\n  - Each change data file will come with a schema file (dfm) that describe the schema of the data file\n  - Azure Eventgrid listen to new files landed in the subscribed folder and create messages detailing locations and type of operations for each file\n  - Our main program will read messages from message queue, sort them by table then process messages in batch with a predefined size. By sorting we will have least number of table possible in each batch\n  - Within each batch, the process_files program will group by table and retrieve a unique schema file and data files for each table in the group by. From schema file, it will form the schema and use it to retrieve data\n  - For insert data, use regular insert. For update and delete, user MERGE to merge data to target table\n2. Libraries: Please install azure-storage-queue and azure to Databricks cluster\n\n3. Data: We use a sample CDC files generated by Attunity. Please upd]ate the sample data to folder where each folder represents a landing location for a table.\n\n4. Configuring Azure Eventgrid and Storage Queue\n\n   https://docs.microsoft.com/en-us/azure/event-grid/custom-event-to-queue-storage\n\n5. Setup Event subscription in your storage account\n\n   Route the event to a Storage Queue\n6. #####TODOs: \n    - Error handling of failures & bad data\n    - Data type change of schema\n    - Deduplication at target table\n    - Optimization of target delta tables"],"metadata":{}},{"cell_type":"code","source":["# Load data from Azure \n# Reset the widgets\n\n\ndbutils.widgets.removeAll()\n\ndbutils.widgets.text(\"STORAGE_ACCOUNT\", \"\")\ndbutils.widgets.text(\"SAS_KEY\", \"\")\ndbutils.widgets.text(\"ACCOUNT_KEY\", \"\")\ndbutils.widgets.text(\"QUEUE_NAME\", \"\")\ndbutils.widgets.text(\"ARCHIVE_QUEUE_NAME\", \"\")\ndbutils.widgets.text(\"INSTRUMENT_KEY\", \"\")\n\ndbutils.widgets.text(\"ROOT_PATH\", \"\")\n\naccount_name = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\nsas=dbutils.widgets.get(\"SAS_KEY\").strip()\naccount_key = dbutils.widgets.get(\"ACCOUNT_KEY\").strip()\nqueue_name=  dbutils.widgets.get(\"QUEUE_NAME\").strip()\narchive_queue_name=  dbutils.widgets.get(\"ARCHIVE_QUEUE_NAME\").strip()\ninstrument_key=dbutils.widgets.get(\"INSTRUMENT_KEY\").strip()\nconf_key = \"fs.azure.account.key.{storage_acct}.dfs.core.windows.net\".format(storage_acct=account_name)\nspark.conf.set(conf_key, account_key)\n\n\nroot_path =dbutils.widgets.get(\"ROOT_PATH\").strip()\n\n#Dictionary of table name and target table path\n \n\nspark.conf.set(\"spark.databricks.delta.schema.autoMerge\", \"true\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## Utility functions to parse schema and load data"],"metadata":{}},{"cell_type":"code","source":["\nfrom delta.tables import *\nfrom pyspark.sql.types import StructField, StructType , LongType, StringType, DoubleType ,DecimalType, DateType,FloatType \n\ndef get_file_info(metadata_file_paths, datafile_basepath=\"\", datafile_extension=\"json\"):\n  \n  \"\"\"\n  Function to parse dfm schema file and return schema for loading data file\n  assumption is each table has its own directory.Inside each directory there're data files and each data file has a metadata file\n  describing schema and other information.\n  Normally in each load, each table will have only one schema but should they have more than 1 schemas due to changes at source, \n  the function has logic to deal with it.\n\n  Parameters\n  --------\n  metadata_file_paths: a list, required\n    a list of full paths to meta data files that contain schema and other information\n  datafile_basepath: a string\n    show the base path down to the last folder containing datafiles\n\n  datafile_extension: a string\n    extension showing type of data file e.g. json or csv..\n  --------\n  \"\"\"\n  schemas =spark.read.option(\"multiLine\", True).option(\"mode\", \"PERMISSIVE\").json(metadata_file_paths)\n  #This map show mapping between oracle data type and Spark SQL. Needs updates to reflect motype\n  ora_pyspark_map = {'STRING':StringType(), 'DATETIME':DateType(),'NUMERIC':DecimalType(), 'REAL8':DecimalType()}\n#   ora_pyspark_map = {'STRING':StringType(), 'DATETIME':DateType(),'NUMERIC':FloatType()}\n\n  schemas = schemas.select([\"dataInfo\", \"fileInfo\"]).collect()\n  #this is a list to contain list of primary keys\n  primary_keys=[]\n  file_schema_mapping={}\n  for num, schema in enumerate(schemas):\n    targetSchema = StructType()\n    for item in schema['dataInfo']['columns']:\n      #default to String type if no mapping is found\n      target_type = ora_pyspark_map.get(item['type'], StringType())\n      if target_type ==DecimalType():\n        scale =int(item['scale'])\n        precision=int(item['precision'])\n        if scale!=0 or precision!=0:\n          target_type =DecimalType(precision,scale)\n        else:\n          target_type =DecimalType(38,10)\n\n      targetSchema.add(item['name'],target_type)\n       #only need to check first schema assuming primary keys won't change \n      if(num==0):\n        if (item['primaryKeyPos']>0):\n          primary_keys.append(item['name'])\n        \n    #build a dict of data file full path to target schema. This can be done because the assumption that data file is\n    #in the same folder as metadata file\n    file_schema_mapping[datafile_basepath+schema['fileInfo']['name']+\".\"+datafile_extension] = targetSchema\n    \n#This algorithm is to build dict of schema:data files (one to many relationship)  \n  schema_datafiles_map= {}\n  for key, value in sorted(file_schema_mapping.items()):\n      schema_datafiles_map.setdefault(value, []).append(key)\n  return schema_datafiles_map,primary_keys\n\n\ndef merge(updatesDF, target_tbl_path,schema,primary_keys, field_names):\n  \"\"\"\n  Function to insert, delete or update data into target tables\n\n  Parameters\n  --------\n  updatesDF: spark dataframe that contains change data to merge\n  target_tbl_path: a string\n   the base path to the target table to merge the updates to\n  schema: a StrucType, optional\n   the schema of target table, may not be needed\n  primary_keys: a list\n   contains a list of primary key(s) needed to delete or update table\n  field_names: a list\n   contains a list of fields of the target table \n  --------\n  \"\"\"\n  #processing the insert\n\n#   updatesDF.cache()\n  insert_df = updatesDF.filter(\"header__change_oper='I'\")\n  #automatic add new columns\n  insert_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(target_tbl_path)\n#   insert_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").save(target_tbl_path)\n  #processing the update/delete\n  upsert_df = updatesDF.filter(\"header__change_oper='U' or header__change_oper='D'\")\n  #build dynamic match condition query\n  update_alias =\"updates\"\n  target_tbl_alias ='tgt_tbl'\n  match_condition=\"\"\n  for num, key in enumerate(primary_keys):\n    if num==0:\n      match_condition= \"{0}.{1} ={2}.{1}\".format(target_tbl_alias, key,update_alias)\n    else:\n      match_condition = match_condition+  \" and {0}.{1} ={2}.{1}\".format(target_tbl_alias, key,update_alias)\n    \n    \n  update_dict ={fieldname: update_alias+\".\"+fieldname for fieldname in field_names}\n\n\n  targetTable = DeltaTable.forName(spark, target_tbl_path)\n  targetTable.alias(target_tbl_alias).merge(\n      upsert_df.alias(update_alias),match_condition\n       ) \\\n    .whenMatchedUpdate(update_alias+\".header__change_oper='U'\",set = update_dict ) \\\n    .whenMatchedDelete(update_alias+\".header__change_oper='D'\") \\\n    .execute()\n  updatesDF.unpersist()\ndef process_files(metadata_files,datafile_basepath,target_tbl_path,tc):\n\n  \"\"\"\n  Function to process multiple data/metadata files and deliver updates/inserts to target tables\n\n  Parameters\n  --------\n  metadata_file_paths: a list, required\n    a list of full paths to meta data files that contain schema and other information\n  datafile_basepath: a string\n    show the base path down to the last folder containing datafiles\n\n  target_tbl_path: a string\n    path to target delta table.\n  --------\n  \"\"\"\n  datafile_extension='csv'\n  zip_extension=\"gz\"\n  chg_tbl_name =\"chg_tbl\"\n  metadata_file_paths=[datafile_basepath+metafile for metafile in metadata_files]\n  schema_datafiles_map,primary_keys = get_file_info(metadata_file_paths,datafile_basepath,zip_extension)\n  for item in schema_datafiles_map.items():\n    datafile_paths = item[1]\n    schema=item[0]\n    print(\"number of files for the table: \", len(datafile_paths))\n    tc.track_metric(\"Number of files\", len(datafile_paths), properties = {\"table\":target_tbl_path})\n     \n#     print(\"schema before read is: \",schema)\n    field_names =[fieldname for fieldname in schema.fieldNames()]\n    data = spark.read.format(datafile_extension).schema(schema).load(datafile_paths)\n#     print(\"schema after read is : \",data.schema)\n    data.createOrReplaceTempView(chg_tbl_name)\n    #generate below query dynamically based on target schema\n    sql_query = \"select {0} from (select {0}, RANK() OVER (PARTITION BY {1} ORDER BY header__change_seq DESC) AS RNK  from {2} ) A where RNK=1\".format(\",\".join(field_names),\",\".join(primary_keys),chg_tbl_name )\n    updatesDF =sql(sql_query)\n    updatesDF.cache()\n    print(\"Files in the update: \", datafile_paths)\n    tc.track_event(\"Files in the update\", {'files': datafile_paths, \"table\":target_tbl_path })\n    updatesize=updatesDF.count()\n    print(\"Size of the updates: \",updatesize )\n    tc.track_metric(\"Size of update\", updatesize, properties = {\"table\":target_tbl_path})\n    merge(updatesDF,target_tbl_path, schema,primary_keys, field_names)\n    tc.track_event('Table processing', { 'table': target_tbl_path })\n    tc.flush()\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Main procedure to process incoming data from Eventgrid"],"metadata":{}},{"cell_type":"code","source":["from azure.storage.queue import QueueService,QueueMessageFormat\nimport ast\nimport time\nfrom applicationinsights import TelemetryClient\ntc = TelemetryClient(instrument_key)\n#Authenticate to Datalake where the files are landed. You can use ABFS or WASB depending on authentication method\n\n\n#Dictionary contain table name and table path maps\ndef main_proc(max_num_tbl=1):\n  #authenticate to the Storeage Queue Service using a shared SAS key\n\n  queue_service = QueueService(account_name=account_name,sas_token=sas)\n\n  #Set visibility_timeout which is the estimated time that your processing will last so that other parallel clusters may not see it. The messages will be back to queue \n  #unless you explicitly delete them which should be done after successful operation. 32 is the max number of messages in one read. If you need more than that, call get_messages \n  #multiple times.\n  #Do this while a while loop so that it keep processing new files\n  batch_size=32\n  batch=0\n  max_messages_explored = 800\n  #initial visibility timeout to filter out messages\n  init_visibility_timeout =60*30\n  #timeout for processing \n  visibility_timeout =2\n  #wait time if current queue is empty before retry\n  wait_time =10\n  while True:\n    dfm_filelist=[]\n    table_list=[]\n    #Get estimate of queue length\n    metadata = queue_service.get_queue_metadata(queue_name)\n    count = metadata.approximate_message_count\n    print(\"Begin processing, entire queue length is \", count)\n    start_queue = time.time()\n    tc.track_event('Queue ingesting', { 'queue_length': count })\n    tc.flush()\n    messages=None\n    messages_ids=[]\n    #This is to get more messages than the default limit of 32\n    while True:\n      batch_messages = queue_service.get_messages(\n            queue_name, num_messages=batch_size, visibility_timeout=init_visibility_timeout)\n\n      if messages is None:\n        messages =  batch_messages\n      else:\n        messages = messages+batch_messages\n        if len(batch_messages)==0 or len(messages)>=max_messages_explored:\n          break\n    #This is the path to append with new files extracted from the queue \n    for message in messages:\n      content =QueueMessageFormat.binary_base64decode(message.content).decode('utf8')\n      json_content = ast.literal_eval(content)  \n      data_content = json_content['data']\n      file_name =data_content['url'].split(\"/\")[-1]\n      tbl_name =data_content['url'].split(\"/\")[-2]\n\n      if ((data_content['api']==\"CopyBlob\" or data_content['api']==\"CreateFile\") and \".dfm\" in file_name and \"__ct\" in tbl_name):\n\n        dfm_filelist.append(file_name)\n        table_list.append(tbl_name)\n        messages_ids.append({\"file_name\":file_name,\"tbl\":tbl_name, \"id\":message.id,\"pop_receipt\":message.pop_receipt,\"content\":message.content})\n    #here is the main processing logic (Data transformation)\n    #1. Reading files, the reader can read multiple files\n    table_dfm_dict = {file:tbl for tbl,file in zip(table_list,dfm_filelist)}\n    dfm_filelist,table_list=[],[]\n    reduced_tbl_file = {}\n    #create a grouping of table:file_list\n    for key, value in sorted(table_dfm_dict.items()):\n        reduced_tbl_file.setdefault(value, []).append(key)\n    \n    #only get certain number of tables to process\n    reduced_tbl_file = dict(list(reduced_tbl_file.items())[0:max_num_tbl])\n#     print(\"reduced table file list is \", reduced_tbl_file)\n    #reset the dfm file list to the files associate with the selected table(s)\n    dfm_filelist = [file for key in reduced_tbl_file.keys() for file in reduced_tbl_file[key]]\n    #now release unused files from list by setting new timeout for files in scope\n    tmp_messages_ids=[]\n    for message in messages_ids:\n      if message[\"file_name\"] in dfm_filelist:\n        tmp_messages_ids.append({\"id\":message['id'],\"pop_receipt\":message['pop_receipt'],\"content\":message['content'],\"tbl\":message['tbl']})\n      else:\n        queue_service.update_message(queue_name, message['id'], message['pop_receipt'], visibility_timeout) \n    messages_ids =tmp_messages_ids\n#     print(\"done updating message timeout, new file list is \",dfm_filelist)\n    filelistlen =len(dfm_filelist)\n    if filelistlen>0:\n      print(\"Start processing \", filelistlen, \" files in this batch\")\n      start_batch = time.time()\n      tc.track_event('start batch processing', { 'number of files': filelistlen })\n      tc.flush()\n      for tbl in reduced_tbl_file.keys():\n#         tbl_path=table_path.get(tbl,root_path+\"/\"+tbl[:-4])\n        if (\"__ct\" in file_name):\n          tbl_path=tbl[:-4]\n        else:\n          tbl_path=tbl\n          \n        tbl_messages_ids=[{\"id\":message['id'],\"pop_receipt\":message['pop_receipt'],\"content\":message['content']} for message in messages_ids if tbl in message['tbl']]\n\n        print(\"start processing: \"+tbl+\" for saving at path: \"+tbl_path)\n        start_tbl = time.time()\n        tc.track_event('Table processing', { 'table': tbl_path })\n        tc.flush()\n        try:\n          process_files(reduced_tbl_file[tbl],root_path+tbl+\"/\", tbl_path,tc)\n          elapsed_tbl_time = time.time() - start_tbl\n          elapsed_queue_time=time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_tbl_time))\n          print(\"Finished processing table {0} in {1}\".format(tbl,elapsed_tbl_time)) \n          for message in tbl_messages_ids:\n            queue_service.delete_message(queue_name, message['id'], message['pop_receipt'])\n            queue_service.put_message(archive_queue_name, message['content'])\n          \n        except Exception as e:\n          print(e)\n          failed_messages= [message['id'] for message in tbl_messages_ids]\n          tc.track_event('exception', { 'error message': str(e) },{ 'failed messageid': str(failed_messages) })\n          tc.flush()\n                                   \n\n      elapsed_batch_time = time.time() - start_batch\n      elapsed_batch_time=time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_batch_time))\n      print(\"finish batch {0}, processed {1} files in {2}\".format(batch, filelistlen,elapsed_batch_time))\n      tc.track_event('finish batch processing', { 'number of files': filelistlen,'duration': elapsed_batch_time })\n      tc.flush()\n      batch=batch+1\n    else:\n      #Wait for messages to arrive\n      elapsed_queue_time = time.time() - start_queue\n      elapsed_queue_time=time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_queue_time))\n      print(\"Finished queue in \",elapsed_queue_time) \n      print(\"Nothing in queue, wait {} seconds for next batch\".format(wait_time))\n      \n      tc.track_event('waiting for new messages', { 'wait_time': wait_time })\n      tc.flush()\n      time.sleep(wait_time)\n      continue \n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["#calling function a single time\nmain_proc(2)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#take 1 table CLARITY.AVAILABILITY, do the merge, table version, origin  change then destination -> see how the original "],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["\nimport threading\n#start 5 jobs simultaneously\nfor i in range(5):\n    t = threading.Thread(target=main_proc)\n    t.start()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10}],"metadata":{"name":"loading_merging_template","notebookId":359646288946156},"nbformat":4,"nbformat_minor":0}
