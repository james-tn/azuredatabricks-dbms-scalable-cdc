{"cells":[{"cell_type":"markdown","source":["##Template for performing event based ingestion and merging from Attunity change files"],"metadata":{}},{"cell_type":"code","source":["#Summary of algorithm\n# - Attunity CDC send change data of various table into ADLS Gen 2 folder, each table has its own folder. \n# - Each change data file will come with a schema file (dfm) that describe the schema of the data file\n# - Eventgrid listen to new files landed in the subscribed folder and create messages  detailing locations and type of operations for each file\n# - Our main program will read messages from message queue, sort them by table then process messages in batch with a predefined size. By sorting we will have least number of table possible in each batch\n# - Within each batch, the process_files progream will group by table and retrieve a unique schema file and data files for each table in the group by. From schema file, it will form the schema and use it to retrieve data  \n# - For insert data, use regular insert. For update and delete, user MERGE to merge data to target table"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Load data from Azure \n# Reset the widgets\ndbutils.widgets.removeAll()\n\ndbutils.widgets.text(\"STORAGE_ACCOUNT\", \"\")\ndbutils.widgets.text(\"SAS_KEY\", \"\")\ndbutils.widgets.text(\"ACCOUNT_KEY\", \"\")\ndbutils.widgets.text(\"QUEUE_NAME\", \"\")\ndbutils.widgets.text(\"ROOT_PATH\", \"\")\n\naccount_name = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\nsas=dbutils.widgets.get(\"SAS_KEY\").strip()\naccount_key = dbutils.widgets.get(\"ACCOUNT_KEY\").strip()\nqueue_name=  dbutils.widgets.get(\"QUEUE_NAME\").strip()\n# 'queue1'\n  # Set up account access key\nconf_key = \"fs.azure.account.key.{storage_acct}.dfs.core.windows.net\".format(storage_acct=account_name)\nspark.conf.set(conf_key, account_key)\n\n# sas='?sv=2019-02-02&ss=bfqt&srt=sco&sp=rwdlacup&se=2020-10-10T23:39:50Z&st=2019-12-19T16:39:50Z&spr=https&sig=1spXbLPp5j4z8A07hUCzJOLdgOZXGhacIW1ot5TqHfQ%3D'\n\nroot_path =dbutils.widgets.get(\"ROOT_PATH\").strip()\ntable_path ={\"test\":\"/tmp/target_test\",\"test2\":\"/tmp/target_test2\"}\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## Utility functions to parse schema and load data"],"metadata":{}},{"cell_type":"code","source":["\nfrom delta.tables import *\nfrom pyspark.sql.types import StructField, StructType , LongType, StringType, DoubleType , DateType \n\ndef get_file_info(paths, file_path=\"\", file_extension=\"json\"):\n#function to parse dfm schema file and return schema for loading data file\n#assumption is each table will have its own directory. All files will be sent here and archiving \n#will output a dictionary of file name and schema\n  schemas =spark.read.option(\"multiLine\", True).option(\"mode\", \"PERMISSIVE\").json(paths)\n  #This map show mapping between oracle data type and Spark SQL. Needs update\n  ora_pyspark_map = {'STRING':StringType(), 'DATETIME':DateType(),'NUMERIC':DoubleType()}\n  schemas = schemas.select([\"dataInfo\", \"fileInfo\"]).collect()\n  output={}\n  for schema in schemas:\n    targetSchema = StructType()\n    for item in schema['dataInfo']['columns']:\n      #default to String type if no mapping is found\n      target_type = ora_pyspark_map.get(item['type'], StringType())\n      targetSchema.add(item['name'],target_type)\n      \n    output[file_path+schema['fileInfo']['name']+\".\"+file_extension] = targetSchema\n  v = {}\n#This algorithm is to produce unique schema info from the list\n  for key, value in sorted(output.items()):\n      v.setdefault(value, []).append(key)\n  return v\ndef get_target_schema(change_schema):\n    targetSchema = StructType()\n    for field in change_schema.fields:\n      if \"header__\" not in field.name:\n        targetSchema.add(field)\n    return targetSchema\n  \n\ndef merge(updatesDF, target_tbl_path):\n#   update_tmp_name = \"updatetbl\"\n#   updatesDF.registerTempTable(update_tmp_name)\n  #processing the insert\n  targetTable = DeltaTable.forPath(spark, target_tbl_path)\n  updatesDF.cache()\n  insert_df = updatesDF.filter(\"header__change_oper='I'\").select(['LOOPNUM', 'TRANSACTIONNUM', 'B', 'C', 'D'])\n  insert_df.write.format(\"delta\").mode(\"append\").save(target_tbl_path)\n  #processing the update/delete\n  upsert_df = updatesDF.filter(\"header__change_oper='U' or header__change_oper='D'\")\n\n  targetTable.alias(\"tgt_tbl\").merge(\n      upsert_df.alias(\"updates\"),\n      \"tgt_tbl.TRANSACTIONNUM = updates.TRANSACTIONNUM and tgt_tbl.LOOPNUM = updates.LOOPNUM\" ) \\\n    .whenMatchedUpdate(\"updates.header__change_oper='U'\",set = {\n                             \"B\" : \"updates.B\",\n                             \"C\" : \"updates.C\",\n                             \"D\" : \"updates.D\"} ) \\\n    .whenMatchedDelete(\"updates.header__change_oper='D'\") \\\n    .execute()\n  \ndef process_files(dfm_filelist,path_prefix,target_tbl_path):\n  data_format='csv'\n  chg_tbl_name =\"test\"\n  file_list = get_file_info(dfm_filelist,path_prefix,data_format)\n  datafilelist=[]\n  for file in file_list.items():\n    path = file[1]\n    schema=file[0]\n    data = spark.read.format(data_format).schema(schema).load(path)\n    data.registerTempTable(chg_tbl_name)\n    #to be implemented: generate below query dynamically based on target schema \n#     updatesDF =sql(\"select header__change_oper, LOOPNUM, TRANSACTIONNUM, B, C, D from (select header__change_oper, LOOPNUM, TRANSACTIONNUM, B, C, D, RANK() OVER (PARTITION BY LOOPNUM, TRANSACTIONNUM,header__change_oper ORDER BY header__change_seq DESC) AS RNK  from \"+chg_tbl_name+\" where header__change_oper='U') A where RNK=1\")\n    updatesDF =sql(\"select header__change_oper, LOOPNUM, TRANSACTIONNUM, B, C, D from (select header__change_oper, LOOPNUM, TRANSACTIONNUM, B, C, D, RANK() OVER (PARTITION BY LOOPNUM, TRANSACTIONNUM ORDER BY header__change_seq DESC) AS RNK  from \"+chg_tbl_name+\" ) A where RNK=1\")\n\n    merge(updatesDF,target_tbl_path)\n  \n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Main procedure to process incoming data from Eventgrid"],"metadata":{}},{"cell_type":"code","source":["from azure.storage.queue import QueueService,QueueMessageFormat\nimport ast\nimport time\n#Authenticate to Datalake where the files are landed. You can use ABFS or WASB depending on authentication method\n\n\n#Dictionary contain table name and table path maps\ndef main_proc():\n  #authenticate to the Storeage Queue Service using a shared SAS key\n\n  queue_service = QueueService(account_name=account_name,sas_token=sas)\n\n  #Set visibility_timeout which is the estimated time that your processing will last so that other parallel clusters may not see it. The messages will be back to queue \n  #unless you explicitly delete them which should be done after successful operation. 32 is the max number of messages in one read. If you need more than that, call get_messages \n  #multiple times.\n  #Do this while a while loop so that it keep processing new files\n\n  batch=0\n  batch_size =32\n  max_bath_num = 5\n  visibility_timeout =5*60\n  #wait time if current queue is empty before retry\n  wait_time =10\n  while True:\n    file_list=[]\n    table_list=[]\n    #Get estimate of queue length\n    metadata = queue_service.get_queue_metadata(queue_name)\n    count = metadata.approximate_message_count\n    print(\"Begining processing, queue length is \", count)\n    messages=None\n    #This is to get more messages than the default limit of 32\n    for i in range(max_bath_num):\n      batch_messages = queue_service.get_messages(\n            queue_name, num_messages=batch_size, visibility_timeout=visibility_timeout)\n      if messages is None:\n        messages = batch_messages\n      else:\n        messages = messages+batch_messages\n    #This is the path to append with new files extracted from the queue \n    for message in messages:\n      content =QueueMessageFormat.binary_base64decode(message.content).decode('utf8')\n      json_content = ast.literal_eval(content)  \n      #THe logic in this example will process anything which is not delete operation including update. Change this to fit your scenario\n      if json_content['data']['api']!=\"DeleteFile\":\n        file_list.append(root_path+\"/\".join(json_content['data']['url'].split(\"/\")[-2:]))\n        table_list.append(json_content['data']['url'].split(\"/\")[-2])\n    #here is the main processing logic (Data transformation)\n    #1. Reading files, the reader can read multiple files\n    dfm_filelist=[item for item in file_list if \".dfm\" in item ]\n    table_dfm_dict = {file:tbl for tbl,file in zip(table_list,dfm_filelist)}\n\n    reduced_tbl_file = {}\n    #create a grouping of table:file_list\n    for key, value in sorted(table_dfm_dict.items()):\n        reduced_tbl_file.setdefault(value, []).append(key)\n    if len(dfm_filelist)>0:\n      for tbl in reduced_tbl_file.keys():\n        tbl_path=table_path.get(tbl,\"/tmp/target_test\")\n        print(\"process: \"+tbl+\" path: \"+tbl_path)\n        process_files(dfm_filelist,root_path+tbl+\"/\", tbl_path)\n      for message in messages:\n        queue_service.delete_message(queue_name, message.id, message.pop_receipt)\n\n      print(\"finish batch {0}, processed {1} files\".format(batch, len(file_list)))\n      batch=batch+1\n    else:\n      #Wait for next batch\n      print(\"Nothing in queue, wait {} seconds for next batch\".format(wait_time))\n      time.sleep(wait_time)\n      continue \n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["#calling function a single time\nmain_proc()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["\nimport threading\n#start 5 jobs simultaneously\nfor i in range(5):\n    t = threading.Thread(target=main_proc)\n    t.start()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["spark.read.format(\"delta\").load(\"/tmp/target_test\").registerTempTable(\"test\")\nspark.read.format(\"delta\").load(\"/tmp/target_test2\").registerTempTable(\"test2\")\n\nsql(\"delete from test2\")\nsql(\"delete from test\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[36]: DataFrame[]</div>"]}}],"execution_count":10},{"cell_type":"code","source":["\n%sql select count(*) from test2\n--checking result"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>0</td></tr></tbody></table></div>"]}}],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"loading_merging_template","notebookId":3389404856211235},"nbformat":4,"nbformat_minor":0}
