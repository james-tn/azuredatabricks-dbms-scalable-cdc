{"cells":[{"cell_type":"markdown","source":["##Template for performing event based ingestion and merging from Attunity change files"],"metadata":{}},{"cell_type":"markdown","source":["Scalable CDC from DBMS to Azure Databricks\n\n1. Summary of algorithm\n\n  - CDC program send change data from various tables into ADLS Gen 2 folder, each table has its own folder.\n  - Each change data file will come with a schema file (dfm) that describe the schema of the data file\n  - Azure Eventgrid listen to new files landed in the subscribed folder and create messages detailing locations and type of operations for each file\n  - Our main program will read messages from message queue, sort them by table then process messages in batch with a predefined size. By sorting we will have least number of table possible in each batch\n  - Within each batch, the process_files program will group by table and retrieve a unique schema file and data files for each table in the group by. From schema file, it will form the schema and use it to retrieve data\n  - For insert data, use regular insert. For update and delete, user MERGE to merge data to target table\n2. Libraries: Please install azure-storage-queue and azure to Databricks cluster\n\n3. Data: We use a sample CDC files generated by Attunity. Please upd]ate the sample data to folder where each folder represents a landing location for a table.\n\n4. Configuring Azure Eventgrid and Storage Queue\n\n   https://docs.microsoft.com/en-us/azure/event-grid/custom-event-to-queue-storage\n\n5. Setup Event subscription in your storage account\n\n   Route the event to a Storage Queue\n6. #####TODOs: \n    - Error handling of failures & bad data\n    - Data type change of schema\n    - Deduplication at target table\n    - Optimization of target delta tables"],"metadata":{}},{"cell_type":"code","source":["# Load data from Azure \n# Reset the widgets\ndbutils.widgets.removeAll()\n\ndbutils.widgets.text(\"STORAGE_ACCOUNT\", \"\")\ndbutils.widgets.text(\"SAS_KEY\", \"\")\ndbutils.widgets.text(\"ACCOUNT_KEY\", \"\")\ndbutils.widgets.text(\"QUEUE_NAME\", \"\")\ndbutils.widgets.text(\"ROOT_PATH\", \"\")\n\naccount_name = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\nsas=dbutils.widgets.get(\"SAS_KEY\").strip()\naccount_key = dbutils.widgets.get(\"ACCOUNT_KEY\").strip()\nqueue_name=  dbutils.widgets.get(\"QUEUE_NAME\").strip()\n\nconf_key = \"fs.azure.account.key.{storage_acct}.dfs.core.windows.net\".format(storage_acct=account_name)\nspark.conf.set(conf_key, account_key)\n\n\nroot_path =dbutils.widgets.get(\"ROOT_PATH\").strip()\n#Dictionary of table name and table path\ntable_path ={\"test\":\"/tmp/target_test\",\"test2\":\"/tmp/target_test2\"}\n\nspark.conf.set(\"spark.databricks.delta.schema.autoMerge\", \"true\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## Utility functions to parse schema and load data"],"metadata":{}},{"cell_type":"code","source":["\nfrom delta.tables import *\nfrom pyspark.sql.types import StructField, StructType , LongType, StringType, DoubleType , DateType,FloatType \n\ndef get_file_info(metadata_file_paths, datafile_basepath=\"\", datafile_extension=\"json\"):\n  \n  \"\"\"\n  Function to parse dfm schema file and return schema for loading data file\n  assumption is each table has its own directory.Inside each directory there're data files and each data file has a metadata file\n  describing schema and other information.\n  Normally in each load, each table will have only one schema but should they have more than 1 schemas due to changes at source, \n  the function has logic to deal with it.\n\n  Parameters\n  --------\n  metadata_file_paths: a list, required\n    a list of full paths to meta data files that contain schema and other information\n  datafile_basepath: a string\n    show the base path down to the last folder containing datafiles\n\n  datafile_extension: a string\n    extension showing type of data file e.g. json or csv..\n  --------\n  \"\"\"\n  schemas =spark.read.option(\"multiLine\", True).option(\"mode\", \"PERMISSIVE\").json(metadata_file_paths)\n  #This map show mapping between oracle data type and Spark SQL. Needs updates to reflect more dtype\n  ora_pyspark_map = {'STRING':StringType(), 'DATETIME':DateType(),'NUMERIC':DoubleType()}\n#   ora_pyspark_map = {'STRING':StringType(), 'DATETIME':DateType(),'NUMERIC':FloatType()}\n\n  schemas = schemas.select([\"dataInfo\", \"fileInfo\"]).collect()\n  #this is a list to contain list of primary keys\n  primary_keys=[]\n  file_schema_mapping={}\n  for num, schema in enumerate(schemas):\n    targetSchema = StructType()\n    for item in schema['dataInfo']['columns']:\n      #default to String type if no mapping is found\n      target_type = ora_pyspark_map.get(item['type'], StringType())\n      targetSchema.add(item['name'],target_type)\n       #only need to check first schema assuming primary keys won't change \n      if(num==0):\n        if (item['primaryKeyPos']>0):\n          primary_keys.append(item['name'])\n        \n    #build a dict of data file full path to target schema. This can be done because the assumption that data file is\n    #in the same folder as metadata file\n    file_schema_mapping[datafile_basepath+schema['fileInfo']['name']+\".\"+datafile_extension] = targetSchema\n    \n#This algorithm is to build dict of schema:data files (one to many relationship)  \n  schema_datafiles_map= {}\n  for key, value in sorted(file_schema_mapping.items()):\n      schema_datafiles_map.setdefault(value, []).append(key)\n  return schema_datafiles_map,primary_keys\ndef get_target_schema(change_schema):\n    targetSchema = StructType()\n    for field in change_schema.fields:\n      if \"header__\" not in field.name:\n        targetSchema.add(field)\n    return targetSchema\n  \n\ndef merge(updatesDF, target_tbl_path,schema,primary_keys, field_names):\n  \"\"\"\n  Function to insert, delete or update data into target tables\n\n  Parameters\n  --------\n  updatesDF: spark dataframe that contains change data to merge\n  target_tbl_path: a string\n   the base path to the target table to merge the updates to\n  schema: a StrucType, optional\n   the schema of target table, may not be needed\n  primary_keys: a list\n   contains a list of primary key(s) needed to delete or update table\n  field_names: a list\n   contains a list of fields of the target table \n  --------\n  \"\"\"\n  #processing the insert\n\n  updatesDF.cache()\n  insert_df = updatesDF.filter(\"header__change_oper='I'\")\n  #automatic add new columns\n  insert_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(target_tbl_path)\n#   insert_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").save(target_tbl_path)\n  #processing the update/delete\n  upsert_df = updatesDF.filter(\"header__change_oper='U' or header__change_oper='D'\")\n  #build dynamic match condition query\n  update_alias =\"updates\"\n  target_tbl_alias =\"tgt_tbl\"\n  match_condition=\"\"\n  for num, key in enumerate(primary_keys):\n    if num==0:\n      match_condition= \"{0}.{1} ={2}.{1}\".format(target_tbl_alias, key,update_alias)\n    else:\n      match_condition = match_condition+  \" and {0}.{1} ={2}.{1}\".format(target_tbl_alias, key,update_alias)\n    \n    \n  update_dict ={fieldname: update_alias+\".\"+fieldname for fieldname in field_names}\n\n  #get the handle to target table, loop_num and transactionnum are  primary keys, B, C, D are fields in the example.\n  targetTable = DeltaTable.forPath(spark, target_tbl_path)\n  targetTable.alias(target_tbl_alias).merge(\n      upsert_df.alias(update_alias),match_condition\n       ) \\\n    .whenMatchedUpdate(update_alias+\".header__change_oper='U'\",set = update_dict ) \\\n    .whenMatchedDelete(update_alias+\".header__change_oper='D'\") \\\n    .execute()\n  \ndef process_files(metadata_file_paths,datafile_basepath,target_tbl_path):\n\n  \"\"\"\n  Function to process multiple data/metadata files and deliver updates/inserts to target tables\n\n  Parameters\n  --------\n  metadata_file_paths: a list, required\n    a list of full paths to meta data files that contain schema and other information\n  datafile_basepath: a string\n    show the base path down to the last folder containing datafiles\n\n  target_tbl_path: a string\n    path to target delta table.\n  --------\n  \"\"\"\n  datafile_extension='csv'\n  chg_tbl_name =\"chg_tbl\"\n  schema_datafiles_map,primary_keys = get_file_info(metadata_file_paths,datafile_basepath,datafile_extension)\n  for item in schema_datafiles_map.items():\n    datafile_paths = item[1]\n    schema=item[0]\n    field_names =[fieldname for fieldname in schema.fieldNames()]\n    data = spark.read.format(datafile_extension).schema(schema).load(datafile_paths)\n    data.createOrReplaceTempView(chg_tbl_name)\n    #generate below query dynamically based on target schema\n    sql_query = \"select {0} from (select {0}, RANK() OVER (PARTITION BY {1} ORDER BY header__change_seq DESC) AS RNK  from {2} ) A where RNK=1\".format(\",\".join(field_names),\",\".join(primary_keys),chg_tbl_name )\n    updatesDF =sql(sql_query)\n    merge(updatesDF,target_tbl_path, schema,primary_keys, field_names)\n  \n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Main procedure to process incoming data from Eventgrid"],"metadata":{}},{"cell_type":"code","source":["from azure.storage.queue import QueueService,QueueMessageFormat\nimport ast\nimport time\n#Authenticate to Datalake where the files are landed. You can use ABFS or WASB depending on authentication method\n\n\n#Dictionary contain table name and table path maps\ndef main_proc():\n  #authenticate to the Storeage Queue Service using a shared SAS key\n\n  queue_service = QueueService(account_name=account_name,sas_token=sas)\n\n  #Set visibility_timeout which is the estimated time that your processing will last so that other parallel clusters may not see it. The messages will be back to queue \n  #unless you explicitly delete them which should be done after successful operation. 32 is the max number of messages in one read. If you need more than that, call get_messages \n  #multiple times.\n  #Do this while a while loop so that it keep processing new files\n\n  batch=0\n  batch_size =32\n  max_batch_num = 5\n  visibility_timeout =5*60\n  #wait time if current queue is empty before retry\n  wait_time =10\n  while True:\n    file_list=[]\n    table_list=[]\n    #Get estimate of queue length\n    metadata = queue_service.get_queue_metadata(queue_name)\n    count = metadata.approximate_message_count\n    print(\"Begining processing, queue length is \", count)\n    messages=None\n    #This is to get more messages than the default limit of 32\n    for i in range(max_batch_num):\n      batch_messages = queue_service.get_messages(\n            queue_name, num_messages=batch_size, visibility_timeout=visibility_timeout)\n      if messages is None:\n        messages = batch_messages\n      else:\n        messages = messages+batch_messages\n    #This is the path to append with new files extracted from the queue \n    for message in messages:\n      content =QueueMessageFormat.binary_base64decode(message.content).decode('utf8')\n      json_content = ast.literal_eval(content)  \n      #THe logic in this example will process anything which is not delete operation including update. Change this to fit your scenario\n      if json_content['data']['api']!=\"DeleteFile\":\n        file_list.append(root_path+\"/\".join(json_content['data']['url'].split(\"/\")[-2:]))\n        table_list.append(json_content['data']['url'].split(\"/\")[-2])\n    #here is the main processing logic (Data transformation)\n    #1. Reading files, the reader can read multiple files\n    dfm_filelist=[item for item in file_list if \".dfm\" in item ]\n    table_dfm_dict = {file:tbl for tbl,file in zip(table_list,dfm_filelist)}\n\n    reduced_tbl_file = {}\n    #create a grouping of table:file_list\n    for key, value in sorted(table_dfm_dict.items()):\n        reduced_tbl_file.setdefault(value, []).append(key)\n    if len(dfm_filelist)>0:\n      for tbl in reduced_tbl_file.keys():\n        tbl_path=table_path.get(tbl,\"/tmp/target_test\")\n        print(\"process: \"+tbl+\" path: \"+tbl_path)\n        process_files(dfm_filelist,root_path+tbl+\"/\", tbl_path)\n      for message in messages:\n        queue_service.delete_message(queue_name, message.id, message.pop_receipt)\n\n      print(\"finish batch {0}, processed {1} files\".format(batch, len(file_list)))\n      batch=batch+1\n    else:\n      #Wait for next batch\n      print(\"Nothing in queue, wait {} seconds for next batch\".format(wait_time))\n      time.sleep(wait_time)\n      continue \n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["#calling function a single time\nmain_proc()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["\nimport threading\n#start 5 jobs simultaneously\nfor i in range(5):\n    t = threading.Thread(target=main_proc)\n    t.start()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["#script to test\nspark.read.format(\"delta\").load(\"/tmp/target_test\").registerTempTable(\"test\")\nspark.read.format(\"delta\").load(\"/tmp/target_test2\").registerTempTable(\"test2\")\n\n# sql(\"delete from test2\")\n# sql(\"delete from test\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["#script to test\n\n%sql select count(*) from test2\n--checking result"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>0</td></tr></tbody></table></div>"]}}],"execution_count":11}],"metadata":{"name":"loading_merging_template","notebookId":3389404856211235},"nbformat":4,"nbformat_minor":0}
